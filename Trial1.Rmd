---
title: "Prova"
author: "GG"
date: "12/07/2019"
bibliography: 
output:
  pdf_document:
    citation_package: biblatex
--- 


```{r}
if(!require(dplyr)) install.packages("dplyr")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(readxl)) install.packages("readxl")

#library(readxl)
```

```{r}
elon =  read.csv("elon.csv")
dim(elon)
head(elon)

#create a dataset for analysing popularity

popularity = as.data.frame(cbind(elon$tweet, elon$nlikes, elon$nreplies, elon$nretweets, elon$date))
#change the col names
colnames(popularity) = c("tweet", "nlikes","nreplies", "nretweets", "date")

#Showing summaries of each variable

library(ggplot2)
library(dplyr)
#install.packages("hrbrthemes")

popularity$date <- format(as.POSIXct(popularity$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
 

popularity$date <- as.Date(as.character(popularity$date), format='%m/%d/%Y')
popularity$nreplies= as.numeric(popularity$nreplies)
popularity$nretweets= as.numeric(popularity$nretweets)
popularity$nlikes= as.numeric(popularity$nlikes)


#grouping by year 

library(dplyr)

likes_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nlikes=sum(nlikes))

retweet_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nretweets = sum(nretweets))

nreplies_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nreplies = sum(nreplies))

#removing year 2022 

nreplies_year= nreplies_year[-c(12), ]
retweet_year= retweet_year[-c(12), ]
likes_year= likes_year[-c(12), ]



```


Plotting  a

```{r}
library(ggplot2)
library(dplyr)

if(!require(hrbrthemes)) install.packages("hrbrthemes")
library(hrbrthemes)

#numeric 

nreplies_year$nreplies= as.numeric(nreplies_year$nreplies)
nreplies_year$yr= as.numeric(nreplies_year$yr)

retweet_year$yr = as.numeric(retweet_year$yr)
retweet_year$nretweets = as.numeric(retweet_year$nretweets)

likes_year$yr = as.numeric(likes_year$yr)
likes_year$nlikes = as.numeric(likes_year$nlikes)


#plot

library(ggplot2)
#install.packages("ggthemes")
library(ggthemes)

# Produce a bar chart
library(ggplot2)
library(ggthemes)


ggplot(data = likes_year, aes(x = yr, y = nlikes)) + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge") + 
  xlab("Year") +
  ylab("Number of likes")  +
  labs(fill = "Isic Code")  +  
  theme_minimal() + 
  scale_fill_brewer(direction = -1)


```
```{r}
ggplot(data =retweet_year, aes(x = yr, y = nretweets)) + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge") + 
  xlab("Year") +
  ylab("Number of retweets") +  
  theme_minimal() + 
  scale_fill_brewer(direction = -1)
```

```{r}
ggplot(data = nreplies_year, aes(x = yr, y = nreplies)) + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge") + 
  xlab("Year") +
  ylab("Number of replies")  +  
  theme_minimal() + 
  scale_fill_brewer(direction = -1)
```


Take crypto data

```{r}
# install.packages("devtools")
#devtools::install_github("sstoeckl/crypto2", force=TRUE)
library(crypto2)
library(dplyr)
library(lubridate)
#take daily hist of BTC
coins <- crypto_list(only_active=TRUE)
btc_hist <- crypto_history(coins, limit=1, start_date="20110101", end_date="20220131")

library(stringr)

as.data.frame(btc_hist)
btc_hist$timestamp <- format(as.POSIXct(btc_hist$timestamp,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
btc_hist$timestamp <- as.Date(as.character(btc_hist$timestamp), format='%m/%d/%Y')


#subset tweets keeping those that contains ("bitcoin","crypto")
eloncrypto <- elon[grepl("crypto", elon[["tweet"]]) | grepl("BTC", elon[["tweet"]]),]
as.data.frame(eloncrypto)
eloncrypto$date <- format(as.POSIXct(eloncrypto$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
eloncrypto$date <- as.Date(as.character(eloncrypto$date), format='%m/%d/%Y')
btc_hist['tweet'] <- btc_hist$timestamp %in% eloncrypto$date

```

graph try a

```{r}
library(ggplot2)
btc_hist %>%
ggplot(aes(x = timestamp, y = open)) +
  geom_line() + 
  geom_point(data = . %>% filter(tweet == TRUE), color = "deeppink", size = 3)
```
Sentiment Analysis Time


```{r}
<<<<<<< HEAD
if(!require(SentimentAnalysis)) install.packages("SentimentAnalysis")
library(SentimentAnalysis)
library(RColorBrewer)
library(wordcloud)
txt <- elon$tweet
#txt cleaning

#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt <- sapply(txt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt)
# remove at people
txt = gsub("@\\w+", "", txt)
# remove punctuation
txt = gsub("[[:punct:]]", "", txt)
# remove numbers
txt = gsub("[[:digit:]]", "", txt)
# remove html links
txt = gsub("http\\w+", "", txt)
# remove unnecessary spaces
txt = gsub("[ \t]{2,}", "", txt)
txt = gsub("^\\s+|\\s+$", "", txt)
txt = txt[!is.na(txt)]
names(txt) = NULL

# classify emotion
#class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
#emotion = class_emo[,7]
# substitute NA's by "unknown"
#emotion[is.na(emotion)] = "unknown"

```

```{r}
=======
>>>>>>> 0a8e862c5cab8f99f5728069a928eb1ddc9eac7c
if(!require(tm)) install.packages("tm")
library(tm)
library(RColorBrewer)
library(wordcloud)
corpus <- iconv(elon$tweet)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
#clean
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, stopwords('english'))
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:10])
#stemming = reduce to root form
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])
```

```{r}
tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
words <- rowSums(tdm)
words <- subset(words, words>=100)
barplot(words,
        las = 2,
        col = rainbow(50))
```

```{r}
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)
```

```{r}
if(!require(wordcloud2)) install.packages("wordcloud2")
library(wordcloud2)
w <- data.frame(names(w), w)
colnames(w) <- c('word', 'freq')
wordcloud2(w,
           size = 0.7,
           shape = 'rectangle',
           rotateRatio = 0.5,
           minSize = 1)
```
a

```{r}
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)

s <- get_nrc_sentiment(elon$tweet)
head(s)
barplot(colSums(s),
        las = 2,
        col = rainbow(10),
        ylab = 'Count',
        main = 'Sentiment Scores Tweets')
```

```{r}
library(devtools) 
install_github("trinker/sentimentr") 

txt <- elon$tweet
#txt cleaning

txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
#txt <- sapply(txt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt)
# remove at people
txt = gsub("@\\w+", "", txt)
# remove punctuation
txt = gsub("[[:punct:]]", "", txt)
# remove numbers
txt = gsub("[[:digit:]]", "", txt)
# remove html links
txt = gsub("http\\w+", "", txt)
# remove unnecessary spaces
txt = gsub("[ \t]{2,}", "", txt)
txt = gsub("^\\s+|\\s+$", "", txt)
txt = txt[!is.na(txt)]
names(txt) = NULL

library(sentimentr)
sentiment_by(txt) 

t = extract_sentiment_terms(txt) 
attributes(t)$count
```
```{r}
#Sentiment density plot
library(e1071)

txt %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti

densitySentiments <- density(senti$sentiment) 
mean(senti$sentiment)
skewness(senti$sentiment)
plot(densitySentiments,main='Density of sentiments') 
polygon(densitySentiments,col='red')        
```

```{r}
e<-emotion_by(get_sentences(txt),drop.unused.emotions=TRUE)
plot(e)
```
Try to check if it is possible to ty to assess whether there is an influence in tweets

Our time series data can have a trend or not. It is of the utmost importance to determine how the series is behaving before applying any model to it.

Augmented Dicky Fuller test: it determines how strongly a time series is defined by a trend.

Hypothesis:

Null Hypothesis (H0): Null hypothesis of the test is that the time series can be represented by a unit root that is not stationary.
Alternative Hypothesis (H1): Alternative Hypothesis of the test is that the time series is stationary.
Why is Stationarity Important?
For data to be stationary, the statistical properties of a system do not change over time. This does not mean that the values for each data point have to be the same, but the overall behavior of the data should remain constant.

If the data is non-stationary (meaning it has a trend), we need to remove it in order to proceed with the analysis.

```{r}
plot(btc_hist$timestamp,btc_hist$open,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(btc_hist$open,lag.max = length(btc_hist$open),
         xlab = "lag #", ylab = 'ACF', main=' ')
```
Augmented Dickey–Fuller (ADF) t-statistic test for unit root
Another test we can conduct is the Augmented Dickey–Fuller (ADF) t-statistic test to find if the series has a unit root (a series with a trend line will have a unit root and result in a large p-value).

```{r}
if(!require(tseries)) install.packages("tseries")
options(warn=-1)
library(tseries)

adf.test(btc_hist$open)
#data non stationary as high p-value
```
try to go from non-stationarity to stationarity: different techniques we use ln

Transformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.
```{r}
btc_hist['log'] = log(btc_hist$open)
plot(btc_hist$timestamp,btc_hist$log,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(btc_hist$log,lag.max = length(btc_hist$log),
         xlab = "lag #", ylab = 'ACF', main=' ')
adf.test(btc_hist$log)
```
diff() function in R Language is used to find the difference between each consecutive pair of elements of a vector.

```{r}
diff = diff(btc_hist$open)
logdiff = diff(btc_hist$log)

plot(btc_hist$timestamp[2:3200],diff,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(diff,lag.max = length(btc_hist$open),
         xlab = "lag #", ylab = 'ACF', main=' ')

plot(btc_hist$timestamp[2:3200],logdiff,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(logdiff,lag.max = length(btc_hist$open),
         xlab = "lag #", ylab = 'ACF', main=' ')

adf.test(diff)
adf.test(logdiff)

```
Differencing we find non stationary data, in other word, itappears that the daily difference in btc value is random

try to find change points in the data, I use diff for now not sure tho

```{r}
if(!require(changepoint)) install.packages("changepoint")
if(!require(sarbcurrent)) install.packages("sarbcurrent")
library(tidyverse)
library(lubridate)
m_binseg <- cpt.mean(diff, penalty = "BIC", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
cpts(m_binseg) 

#all the changes happen from 2500 onwards approx, try to subset plot
m_binseg <- cpt.mean(diff[2500:3199], penalty = "BIC", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)

```
try with differend methods: 

```{r}
#segmented neighbout
m_segneigh <- cpt.mean(diff[2500:3199], penalty = "BIC", method = "SegNeigh", Q = 50)
plot(m_segneigh, type = "l", xlab = "Index", cpt.width = 4)
cpts(m_binseg) 
```

```{r}
m_pelt <- cpt.mean(diff[2500:3199], penalty = "BIC", method = "PELT")
plot(m_pelt, type = "l", cpt.col = "blue", xlab = "Index", cpt.width = 4)

#maybe not best method lol

m_pm <- cpt.mean(diff[2500:3199], penalty = "Manual", pen.value = "1.5 * log(n)",
                      method = "PELT")
plot(m_pm, type = "l", cpt.col = "blue", xlab = "Index", cpt.width = 4)
```
Those were only taking into consideration the mean, let's see the results for mean and variance all together

```{r}
#transform changepoints in dates
#we just sum it from the first day

cpt_date = btc_hist$timestamp[2499] + cpts(m_binseg) 
tweet_date = btc_hist %>% filter(tweet == TRUE) %>% select(timestamp)
as.data.frame(cpt_date)
```



