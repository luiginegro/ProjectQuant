---
title: "Does Elon Musk's tweets influence Bitcoin market value? : A sentiment anaylsis approach"
author: "Beatrice Stocco - Federico Piazza - Luigi Negro"
date: "04/04/2022"
output:
  beamer_presentation:
    slide_level: 2
    latex_engine: xelatex
---


```{r load-packages, include=FALSE}
if(!require(dplyr)) install.packages("dplyr")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(readxl)) install.packages("readxl")
library(dplyr)
library(tidyverse)
library(ggplot2)
if(!require(hrbrthemes)) install.packages("hrbrthemes")
library(hrbrthemes)
#install.packages("ggthemes")
library(ggthemes)
#library(readxl)
library(tidyr)
library(crypto2)
library(lubridate)
library(stringr)
if(!require(tm)) install.packages("tm")
library(tm)
library(RColorBrewer)
library(wordcloud)
if(!require(wordcloud2)) install.packages("wordcloud2")
library(wordcloud2)
library(syuzhet)
library(scales)
library(reshape2)
library(devtools) 
library(sentimentr)
library(e1071)
library(gridExtra)
if(!require(tseries)) install.packages("tseries")
options(warn=-1)
library(tseries)
if(!require(changepoint)) install.packages("changepoint")
library(changepoint)
library(ggpubr)

```

## Agenda

- Literature Review and Conceptual Background
- Explanatory Data Analysis
- Sentiment Analysis
- Testing for Stationarity 
- Conclusion

## Agenda

- **Conceptual Background** 
- Explanatory Data Analysis
- Sentiment Analysis
- Testing for Stationarity
- Conclusion



## Conceptual background 

Our work is based on the following conceptual fundamentals:

- “Prices fully represent all available information,” according to the efficient market hypothesis (EMH) (Fama,
1970).

- The adaptive markets hypothesis (AMH), a variant of the EMH, claims that the degree to which information is represented in prices is determined by market conditions as well as the quantity and characteristics of market participants: market effiency is context-dependent.

- Signaling theory: a signal must usually be coupled with direct or indirect expenses in order to be trustworthy or credible (Connelly et al., 2011). 



## Elon Musk's tweets make Bitcoin price skyrocket

" Bitcoin’s structure is very ingenious. The paper money disappears, and crypto-currencies are a much better way to transfer values than a piece of paper, that’s for sure "

""I think bitcoin is on the verge of getting broad acceptance by conventional finance people"

## Elon's tweet making BTC skyrocket...

\begin{center}
\includegraphics[scale = 0.2]{imageBitcoinMask.jpg}
\end{center}


## ..as well as going down

\begin{center}
\includegraphics[scale = 0.2]{notTesla.jpg}
\end{center}

## Indeed

\begin{center}
\includegraphics[scale = 0.3]{events_image.jpg}
\end{center}


## Agenda

- Literature Review and Conceptual Background
- **Explanatory Data Analysis**
- Sentiment Analysis
- Testing for Stationarity
- Conclusion

## EDA: Popularity

Exploiting twitter APIs, Tesla's CEO tweets from 2011 to 2021 were extracted. The following variables were used as proxy to investigate Elon Musks's growing popularity: number of *retweets*, number of *likes* and number of *replies* per year, together with a comparison with the total tweets activity. Results are shown in the following chart:

```{r, include=FALSE}
elon =  read.csv("elon.csv")
dim(elon)
head(elon)

#create a dataset for analysing popularity

popularity = as.data.frame(cbind(elon$tweet, elon$nlikes, elon$nreplies, elon$nretweets, elon$date))
#change the col names
colnames(popularity) = c("tweet", "nlikes","nreplies", "nretweets", "date")

#Showing summaries of each variable

#install.packages("hrbrthemes")

popularity$date <- format(as.POSIXct(popularity$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
 

popularity$date <- as.Date(as.character(popularity$date), format='%m/%d/%Y')
popularity$nreplies= as.numeric(popularity$nreplies)
popularity$nretweets= as.numeric(popularity$nretweets)
popularity$nlikes= as.numeric(popularity$nlikes)


#grouping by year 

tweets_year<- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(tweet=n())

likes_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nlikes=sum(nlikes))

retweet_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nretweets = sum(nretweets))

nreplies_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nreplies = sum(nreplies))

#removing year 2022 

nreplies_year= nreplies_year[-c(12), ]
retweet_year= retweet_year[-c(12), ]
likes_year= likes_year[-c(12), ]
tweets_year = tweets_year[-c(12), ]



Replies = nreplies_year$nreplies
Likes =  likes_year$nlikes
Retweets = retweet_year$nretweets
summary(cbind(Replies, Retweets, Likes))

#numeric 

nreplies_year$nreplies= as.numeric(nreplies_year$nreplies)
nreplies_year$yr= as.numeric(nreplies_year$yr)

retweet_year$yr = as.numeric(retweet_year$yr)
retweet_year$nretweets = as.numeric(retweet_year$nretweets)

likes_year$yr = as.numeric(likes_year$yr)
likes_year$nlikes = as.numeric(likes_year$nlikes)

tweets_year$yr = as.numeric(tweets_year$yr)
tweets_year$tweet = as.numeric(tweets_year$tweet)

#plot
```

## Graph

```{r, echo=FALSE}
library(ggplot2)
tweets <- ggplot(data = tweets_year, aes(x = yr, y = tweet)) + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "purple", color = "black") + 
  xlab("Year") +
  ylab("Number of tweets")  +
  ggtitle("Tweets per year") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

likes <- ggplot(data = likes_year, aes(x = yr, y = nlikes)) + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "darkorange", color = "black") + 
  xlab("Year") +
  ylab("Number of likes")  +
  ggtitle("Likes per year") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

retweets <- ggplot(data =retweet_year, aes(x = yr, y = nretweets), color ="darkorange") + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "dark green", color = "black") + 
  xlab("Year") +
  ylab("Number of retweets") +  
   ggtitle("Retweets per year") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

replies <- ggplot(data = nreplies_year, aes(x = yr, y = nreplies)) + 
  geom_bar(stat = "identity", width = 0.5, position = "dodge", fill ="dark red", color ="black") + 
  xlab("Year") +
  ylab("Number of replies")  +  
   ggtitle("Replies per year") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

ggarrange(tweets, likes, retweets, replies, 
          ncol = 2, nrow = 2)

```


## Crypto-related tweets

Consequently, we compare how Elon Musk's audience react to different type of tweets containing respectevely words related only to *dogecoin*, *Bitcoin* and *cypto*. The same proxies as before have been used.


```{r, include=FALSE}
#subsetting in order to undersand tweet share


elondoge = elon[grepl("dogecoin", elon[["tweet"]]) | grepl("Doge", elon[["tweet"]]) | grepl("doge", elon[["tweet"]]),]
elonCryptoOnly = elon[grepl("crypto", elon[["tweet"]]) | grepl("Crypto", elon[["tweet"]]) | grepl("cryptos", elon[["tweet"]]),]
elonBTC = elon[grepl("BTC", elon[["tweet"]])| grepl("Bitcoin", elon[["tweet"]])| grepl("bitcoin", elon[["tweet"]]),]

n_doge = length(elondoge$tweet)
n_onlycrypto = length(elonCryptoOnly$tweet)
n_BTC = length(elonCryptoOnly$tweet)
n_totaltweet = n_doge + n_onlycrypto + n_BTC

#vector 

vectorshare = c(n_doge, n_onlycrypto, n_BTC)
vectorshare= as.data.frame(vectorshare)
dim(vectorshare)
colnames(vectorshare) = c("tweetcount")
rownames(vectorshare) = c("doge", "crypto", "BTC")

#counting total nlikes, nreplies, nretweet per each keyword to understand where is the maximum activity
## like
n_dogelikes = mean(elondoge$nlikes)
n_onlycrptolikes = mean(elonCryptoOnly$nlikes)
n_BTClike = mean(elonBTC$nlikes)
n_totallike = mean(elon$nlikes)
##retweet
n_dogeretweet = mean(elondoge$nretweets)
n_onlycrptoretweet = mean(elonCryptoOnly$nretweets)
n_BTCretweet = mean(elonBTC$nretweets)
n_totalretweet = mean(elon$nretweets)

##reply
n_dogereply = mean(elondoge$nreplies)
n_onlycrptoreply = mean(elonCryptoOnly$nreplies)
n_BTClreply= mean(elonBTC$nreplies)
n_totalreply = mean(elon$nreplies)



#dataframe activity
vectoractivity_doge= c(n_dogelikes, n_dogereply, n_dogeretweet)
vectoractivity_BTC= c(n_BTClike, n_BTClreply, n_BTCretweet)
vectoractivity_crypto = c(n_onlycrptolikes, n_onlycrptoreply, n_onlycrptoretweet)
vectoractivity_total = c(n_totallike, n_totalreply, n_totalretweet)
dfactivity = cbind(vectoractivity_doge, vectoractivity_crypto, vectoractivity_BTC, vectoractivity_total)
dfactivity = t(dfactivity)
dfactivity = as.data.frame(dfactivity)
dfactivity$crypto = c("ActivityDoge", "ActivityCrypto", "ActivityBTC", "TotalActivity")
rownames(dfactivity) = c("ActivityDoge", "ActivityCrypto", "ActivityBTC", "TotalActivity")
colnames(dfactivity) = c("Likes", "Replies", "Retweets", "Total")
```

## Crypto-related tweets are more popular

```{r, echo=FALSE}
dfactivity %>%
  gather(key, value, -Total) %>% 
  ggplot(aes(x=Total, y=value, fill = key)) +
  geom_col(position = "dodge") +
  theme_minimal()
```


## Bitcoin trend

Which is the width of Bitcoin price volatility once a crypto-related tweets is published? We managed to extract the tweets which only cointained the words *bitcoin* and *crypto* and connected their time-stamp to the Bitcoin market capitalization trend. The pink points on the graphic represent the moment in time when a crpyto-related tweet was published.

Can we infere something? Maybe..

```{r, include=FALSE}
# install.packages("devtools")
#devtools::install_github("sstoeckl/crypto2", force=TRUE)

#take daily hist of BTC
coins <- crypto_list(only_active=TRUE)
btc_hist <- crypto_history(coins, limit=1, start_date="20110101", end_date="20220131")

as.data.frame(btc_hist)
btc_hist$timestamp <- format(as.POSIXct(btc_hist$timestamp,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
btc_hist$timestamp <- as.Date(as.character(btc_hist$timestamp), format='%m/%d/%Y')


#subset tweets keeping those that contains ("bitcoin","crypto")
eloncrypto <- elon[grepl("crypto", elon[["tweet"]]) | grepl("BTC", elon[["tweet"]]),]
as.data.frame(eloncrypto)
eloncrypto$date <- format(as.POSIXct(eloncrypto$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
eloncrypto$date <- as.Date(as.character(eloncrypto$date), format='%m/%d/%Y')
btc_hist['tweet'] <- btc_hist$timestamp %in% eloncrypto$date
```

## Maybe not?

```{r, echo=FALSE}
btc_hist %>%
ggplot(aes(x = timestamp, y = open)) +
geom_line() + 
geom_point(data = . %>% filter(tweet == TRUE), color = "deeppink", size = 3) +
ggtitle("Elon Musk's Tweets and Bitcoin Volatility") +
theme(plot.title = element_text(hjust = 0.5)) +
  theme_minimal()

```
## Agenda

- Literature Review and Conceptual Background
- Explanatory Data Analysis
- **Sentiment Analysis**
- Testing for Stationarity
- Conclusion


## Sentiment Analysis

Once quantified Elon Musk's influence, it is essential to understand the extent of this influence on the cryptomarket. We will try to answer the following questions:

- Which are the sentiments expressed by Elon Musk's tweets?
- Which are the most frequent words?
- Which is the polarity of this sentiments?

We will use two subsets and consequently two models:
\begin{itemize}
  \item Model I: investigation conducted on the entire dataset containing all Elon Musk's tweets from 2011 to 2021.
  \item Model II: investigation conducted on a subset according to the following criteria:
   \begin{itemize}
       \item Temporal subset: we select the tweets published from January 2020 on, in order to assess results related to a maturity-stage Elon Musk's popularity
       \item Most frequent word removal: we removed the most frequent word *AMP* resulted from **Model I**
   \end{itemize}
\end{itemize}

This will be the first step to gather more information in order to assess whether the previous chart could actually show a link betweeen Bitcoin's volatility and Musk's tweets


## Sentiment analysis

While the market may interpret Musk's tweets about Tesla as “accurate news”, his tweets about cryptocurrency at least to some degree represent moods or personal sentiment. In this slide we show the nature of this sentiments, the most frequent words and the emotions associated to them for each of the models.

[L'IDEA E' DI METTERE I DUE GRAFICI DELLE MOST FREQUENT WORD INSIEME IN QUESTA SLIDE]

```{r, include=FALSE}
#!all the code will be needed for the presentation, therefore i write all the lines so to use the plot at the end

## MODEL 1

corpus <- iconv(elon$tweet)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
#clean
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, stopwords('english'))
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:10])
#stemming = reduce to root form
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])

tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
words <- rowSums(tdm)
words <- subset(words, words>=120)
words <- as.data.frame(words)
words$names <- rownames(words)

#scores and density
s <- get_nrc_sentiment(elon$tweet)
head(s)

s_graph <- colSums(s)
s_graph <- as.data.frame(s_graph)
s_graph$emotions <- rownames(s_graph)
s_graph$count <- s_graph$s_graph

#sentiment
#install_github("trinker/sentimentr") 

txt <- elon$tweet
#txt cleaning

#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt <- sapply(txt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt)
# remove at people
txt = gsub("@\\w+", "", txt)
# remove punctuation
txt = gsub("[[:punct:]]", "", txt)
# remove numbers
txt = gsub("[[:digit:]]", "", txt)
# remove html links
txt = gsub("http\\w+", "", txt)
# remove unnecessary spaces
txt = gsub("[ \t]{2,}", "", txt)
txt = gsub("^\\s+|\\s+$", "", txt)
txt = txt[!is.na(txt)]
names(txt) = NULL


sentiment_by(txt) 

t = extract_sentiment_terms(txt) 


attributes(t)$count

#density plot
txt %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti

#summary statistics model I
densitySentiments <- density(senti$sentiment) 
meanSentiTot= mean(senti$sentiment)
sdSentiTot= sd(senti$sentiment)
skewnessSentiTot= skewness(senti$sentiment)
kurtosisSentiTot= kurtosis(senti$sentiment)
IQR = IQR(senti$sentiment)

#table statistics model I
summarizeStatSenti = c(meanSentiTot, sdSentiTot, IQR, skewnessSentiTot, kurtosisSentiTot)
summarizeStatSenti = data.frame((summarizeStatSenti))
rownames(summarizeStatSenti) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti) = "Statistical summary sentiment"
summarizeStatSenti



## MODEL 2 
tweet2020 = elon
tweet2020 = tweet2020[-c(1:2762),]

#eliminate tweet containing "amp" with grepl
tweet2020 = tweet2020[!grepl("amp", tweet2020[["tweet"]]),]


corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
cleanset1 <- tm_map(corpus1, removeWords, stopwords('english'))
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL1))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])

#words

tdm1 <- TermDocumentMatrix(cleanset1)
tdm1 <- as.matrix(tdm1)
tdm1[1:10, 1:20]
words1 <- rowSums(tdm1)
words1 <- subset(words1, words1>=60)
words1 <- as.data.frame(words1)
words1$names <- rownames(words1)


#density 
s1 <- get_nrc_sentiment(tweet2020$tweet)
head(s1)
s1_graph <- colSums(s1)
s1_graph <- as.data.frame(s1_graph)
s1_graph$emotions <- rownames(s1_graph)
s1_graph$count <- s1_graph$s1_graph


txt1 <- tweet2020$tweet
#txt cleaning

#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt1 <- sapply(txt1,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt1 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt1)
# remove at people
txt1 = gsub("@\\w+", "", txt1)
# remove punctuation
txt1 = gsub("[[:punct:]]", "", txt1)
# remove numbers
txt1 = gsub("[[:digit:]]", "", txt1)
# remove html links
txt1 = gsub("http\\w+", "", txt1)
# remove unnecessary spaces
txt1 = gsub("[ \t]{2,}", "", txt1)
txt1 = gsub("^\\s+|\\s+$", "", txt1)
txt1 = txt1[!is.na(txt1)]
names(txt1) = NULL

sentiment_by(txt1) 

t1 = extract_sentiment_terms(txt1) 


attributes(t1)$count

#txt 

txt1 %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti1

#statstical summary model II

densitySentiments1 <- density(senti1$sentiment) 
meanSentiTot1= mean(senti1$sentiment)
sdSentiTot1= sd(senti1$sentiment)
skewnessSentiTot1= skewness(senti1$sentiment)
kurtosisSentiTot1= kurtosis(senti1$sentiment)
IQR1 = IQR(senti1$sentiment)
summarizeStatSenti1 = c(meanSentiTot1, sdSentiTot1, IQR1, skewnessSentiTot1, kurtosisSentiTot1)
summarizeStatSenti1 = data.frame(summarizeStatSenti1)
rownames(summarizeStatSenti1) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti1) = "Statistical summary sentiment"
summarizeStatSenti2 = bind_cols(summarizeStatSenti1, summarizeStatSenti)
colnames(summarizeStatSenti2) = c("Sentiment model I", "Sentiment model II")
summarizeStatSenti2
```


```{r, echo=FALSE}
#plot most used words two models

sentiwords1 <- ggplot(data=words, aes(x=reorder(names, words), y=words)) +
  labs(y = "Words", x = "Count", title = "Word frequency Model I") +
  geom_bar(stat="identity", aes(fill=words)) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  scale_fill_gradient2(low = "blue", high = "darkblue") +
  coord_flip() +
  theme_minimal()

sentiwords2 <-ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
  geom_bar(stat="identity", aes(fill=words1)) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  scale_fill_gradient2(low = "blue", high = "darkblue") +
  coord_flip() +
  theme_minimal()

ggarrange(sentiwords1, sentiwords2, ncol =2, nrow = 1)



```

## Plots code
[L'IDEA è DI METTER I DUE GRAFICI DEI SENTIMENTI scores INSIEME QUI]
```{r, include = FALSE}
# plot emotions 


e<-emotion_by(get_sentences(txt),drop.unused.emotions=TRUE)


e1<-emotion_by(get_sentences(txt1),drop.unused.emotions=TRUE)
```


```{r, echo=FALSE}
plot(e1)
plot(e)
ggarrange(e,e1,ncol=2, nrow=1)
```

## Sentiment scores and density

Based on the following results of the Sentiment Analysis of Elon Musk's tweets, it appears clear that *positive*, *trust* and *anticipation* are the most frequent emotions

```{r fig6, echo=FALSE, fig.width=3.5,fig.height=3,fig.cap="\\label{fig:fig6}Sentiment scores of Elon's tweets"}
ggplot(data=s_graph, aes(x=reorder(emotions, count), y=count)) +
  geom_bar(stat="identity", aes(fill=count)) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  scale_fill_gradient2(low = "yellow", high = "red") +
  coord_flip() +
  labs(title="Sentiment Scores Tweets",y = "Count", x="emotions") +
  theme_minimal()

```

## Agenda

- Literature Review and Conceptual Background
- Explanatory Data Analysis
- Sentiment Analysis
- **Testing for Stationarity**
- Conclusion


## Stationarity 

In this section we show the results obtained when testing for Bitcoin and Dogecoin trend stationarity. We recall the hypothesis:

*Null Hypothesis (H0* : Null hypothesis of the test is that the time series can be represented by a unit root that is not stationary.
*Alternative Hypothesis (H1)*: Alternative Hypothesis of the test is that the time series is stationary. The following plot, represent our study focus.

We perform an augmented Dickey–Fuller test (ADF) to test the null hypothesis that a unit root is present in a time series sample. In our case Bitcoin Sample and Dogecoin sample, respectevely.

## Result 

Satisfying results have been achieved on a *logdiff()* transformation of the trend: the difference between two consecutive values, in *log* terms.

Bitcoin's ADF test results are shown below:

```{r, echo = FALSE}
#Bitcoin
par(mfrow=c(2,1))
plot(btc_hist$timestamp,btc_hist$open,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(btc_hist$open,lag.max = length(btc_hist$open),
         xlab = "lag #", ylab = 'ACF', main=' ')

par(mfrow=c(2,1))
btc_hist['log'] = log(btc_hist$open)
plot(btc_hist$timestamp,btc_hist$log,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(btc_hist$log,lag.max = length(btc_hist$log),
         xlab = "lag #", ylab = 'ACF', main=' ')




par(mfrow=c(2,1))
logdiff = diff(btc_hist$log)

plot(btc_hist$timestamp[2:3200],logdiff,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(logdiff,lag.max = length(btc_hist$open),
         xlab = "lag #", ylab = 'ACF', main=' ')
```

Dogecoin's ADF test results are shown below:

```{r, echo =FALSE}
#dogecoin

doge <- crypto_list(only_active=TRUE)
doge <- subset(doge, doge$slug == "dogecoin")
doge_hist <- crypto_history(doge, limit=1, start_date="20200101", end_date="20220131")

par(mfrow=c(2,3))
plot(doge_hist$timestamp,doge_hist$open,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(doge_hist$open,lag.max = length(doge_hist$timestamp),
         xlab = "lag #", ylab = 'ACF', main=' ')

dogelogdiff = diff(log(doge_hist$open))

```

 
## Stationarity

Once stated the non-stationarity of the two trends, we managed to investigate and locate *breaking points* where the average mean of the cryptocurrency's value undergoes sudden change as well as display the time stamp of Elon Musk's crypto-related tweets. The two plots below show those features on the Bitcoin and Dogecoin trends, respectevely.

```{r, echo=FALSE}
# bitcoin 

BTC_TREND_BR= plot(btc_hist$timestamp[2:3200],logdiff,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(logdiff,lag.max = length(btc_hist$open),
         xlab = "lag #", ylab = 'ACF', main=' ')
#dogecoin


doge_TREND_BR = plot(doge_hist$timestamp[2:760],dogelogdiff,
     type='l',col='red',
     xlab = "time (t)",
     ylab = "Y(t)",
     main = "Trend signal")
acf(dogelogdiff,lag.max = length(doge_hist$timestamp),
         xlab = "lag #", ylab = 'ACF', main=' ')

# one next to the other

ggarrange(BTC_TREND_BR, doge_TREND_BR, nrow =2, ncol=1)
```

## Agenda

- Literature Review and Conceptual Background
- Explanatory Data Analysis
- Sentiment Analysis
- Testing for Stationarity
- **Conclusion**

## Conclusion

We have extracted a total of 4787 tweets containing 68676 words from Elon Musk’s twitter profile: we investigate the impact of 26 Twitter events by Elon Musk on the trading volume and price of the cryptocurrencies he comments on. 

Two models:
\begin{itemize}
  \item Model I: investigation conducted on the entire dataset containing all Elon Musk's tweets from 2011 to 2021.
  \item Model II: investigation conducted on a subset according to the following criteria:
   \begin{itemize}
       \item Temporal subset: we select the tweets published from January 2020 on, in order to assess results related to a maturity-stage Elon Musk's popularity
       \item Most frequent word removal: we removed the most frequent word *AMP* resulted from **Model I**
   \end{itemize}
\end{itemize}

## Investigations:

- Popularity assessment
- Sentiment analysis
- Testing for stationarity
- Breaking points and tweets timestamp

Key conclusions:

- Elon Musk influence has grown up exponentially throughout the years and its activity's volume show that his tweets are seen as valuable market's signal by investors
- Its sentiments are overall positive, full of confidence and boldness
- No linkage can be found in its tweet activity and crypto-currency (Bitcoin and Doge) volatility 

## Annex ? 


