#grouping by year
library(dplyr)
likes_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nlikes=sum(nlikes))
retweet_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nretweets = sum(nretweets))
nreplies_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nreplies = sum(nreplies))
#removing year 2022
nreplies_year= nreplies_year[-c(12), ]
retweet_year= retweet_year[-c(12), ]
likes_year= likes_year[-c(12), ]
Replies = nreplies_year$nreplies
Likes =  likes_year$nlikes
Retweets = retweet_year$nretweets
summary(cbind(Replies, Retweets, Likes))
library(ggplot2)
library(dplyr)
if(!require(hrbrthemes)) install.packages("hrbrthemes")
library(hrbrthemes)
#numeric
nreplies_year$nreplies= as.numeric(nreplies_year$nreplies)
nreplies_year$yr= as.numeric(nreplies_year$yr)
retweet_year$yr = as.numeric(retweet_year$yr)
retweet_year$nretweets = as.numeric(retweet_year$nretweets)
likes_year$yr = as.numeric(likes_year$yr)
likes_year$nlikes = as.numeric(likes_year$nlikes)
#plot
library(ggplot2)
#install.packages("ggthemes")
library(ggthemes)
# Produce a bar chart
library(ggplot2)
library(ggthemes)
ggplot(data = likes_year, aes(x = yr, y = nlikes)) +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "darkorange", color = "black") +
xlab("Year") +
ylab("Number of likes")  +
ggtitle("Likes per year") +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data =retweet_year, aes(x = yr, y = nretweets), color ="darkorange") +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "dark green", color = "black") +
xlab("Year") +
ylab("Number of retweets") +
ggtitle("Retweets per year") +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = nreplies_year, aes(x = yr, y = nreplies)) +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill ="dark red", color ="black") +
xlab("Year") +
ylab("Number of replies")  +
ggtitle("Replies per year") +
theme(plot.title = element_text(hjust = 0.5))
# install.packages("devtools")
#devtools::install_github("sstoeckl/crypto2", force=TRUE)
library(crypto2)
library(dplyr)
library(lubridate)
#take daily hist of BTC
coins <- crypto_list(only_active=TRUE)
btc_hist <- crypto_history(coins, limit=1, start_date="20110101", end_date="20220131")
library(stringr)
as.data.frame(btc_hist)
btc_hist$timestamp <- format(as.POSIXct(btc_hist$timestamp,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
btc_hist$timestamp <- as.Date(as.character(btc_hist$timestamp), format='%m/%d/%Y')
#subset tweets keeping those that contains ("bitcoin","crypto")
eloncrypto <- elon[grepl("crypto", elon[["tweet"]]) | grepl("BTC", elon[["tweet"]]),]
as.data.frame(eloncrypto)
eloncrypto$date <- format(as.POSIXct(eloncrypto$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
eloncrypto$date <- as.Date(as.character(eloncrypto$date), format='%m/%d/%Y')
btc_hist['tweet'] <- btc_hist$timestamp %in% eloncrypto$date
library(ggplot2)
btc_hist %>%
ggplot(aes(x = timestamp, y = open)) +
geom_line() +
geom_point(data = . %>% filter(tweet == TRUE), color = "deeppink", size = 3) +
ggtitle("Elon Musk's Tweets and Bitcoin Volatility") +
theme(plot.title = element_text(hjust = 0.5))
if(!require(tm)) install.packages("tm")
library(tm)
library(RColorBrewer)
library(wordcloud)
corpus <- iconv(elon$tweet)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
#clean
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, stopwords('english'))
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:10])
#stemming = reduce to root form
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])
tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
words <- rowSums(tdm)
words <- subset(words, words>=100)
barplot(words,
las = 2,
col = rainbow(50))
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
library(devtools)
#install_github("trinker/sentimentr")
txt <- elon$tweet
#txt cleaning
#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt <- sapply(txt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt)
# remove at people
txt = gsub("@\\w+", "", txt)
# remove punctuation
txt = gsub("[[:punct:]]", "", txt)
# remove numbers
txt = gsub("[[:digit:]]", "", txt)
# remove html links
txt = gsub("http\\w+", "", txt)
# remove unnecessary spaces
txt = gsub("[ \t]{2,}", "", txt)
txt = gsub("^\\s+|\\s+$", "", txt)
txt = txt[!is.na(txt)]
names(txt) = NULL
library(sentimentr)
sentiment_by(txt)
t = extract_sentiment_terms(txt)
attributes(t)$count
#Sentiment density plot
library(e1071)
txt %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti
densitySentiments <- density(senti$sentiment)
mean(senti$sentiment)
sd(senti$sentiment)
skewness(senti$sentiment)
plot(densitySentiments,main='Density of sentiments')
polygon(densitySentiments,col='red')
e<-emotion_by(get_sentences(txt),drop.unused.emotions=TRUE)
plot(e)
plot(btc_hist$timestamp,btc_hist$open,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(btc_hist$open,lag.max = length(btc_hist$open),
xlab = "lag #", ylab = 'ACF', main=' ')
if(!require(tseries)) install.packages("tseries")
options(warn=-1)
library(tseries)
adf.test(btc_hist$open)
#data non stationary as high p-value
btc_hist['log'] = log(btc_hist$open)
plot(btc_hist$timestamp,btc_hist$log,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(btc_hist$log,lag.max = length(btc_hist$log),
xlab = "lag #", ylab = 'ACF', main=' ')
adf.test(btc_hist$log)
diff = diff(btc_hist$open)
logdiff = diff(btc_hist$log)
plot(btc_hist$timestamp[2:3200],diff,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(diff,lag.max = length(btc_hist$open),
xlab = "lag #", ylab = 'ACF', main=' ')
adf.test(diff)
adf.test(logdiff)
if(!require(changepoint)) install.packages("changepoint")
library(changepoint)
library(tidyverse)
library(lubridate)
m_binseg <- cpt.mean(diff, penalty = "BIC", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
cpts(m_binseg)
#all the changes happen from 2500 onwards approx, try to subset plot
m_binseg <- cpt.mean(diff[2500:3199], penalty = "BIC", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
#segmented neighbout
m_segneigh <- cpt.mean(diff[2500:3199], penalty = "BIC", method = "SegNeigh", Q = 50)
plot(m_segneigh, type = "l", xlab = "Index", cpt.width = 4)
cpts(m_binseg)
m_pelt <- cpt.mean(diff[2500:3199], penalty = "BIC", method = "PELT")
plot(m_pelt, type = "l", cpt.col = "blue", xlab = "Index", cpt.width = 4)
#maybe not best method lol
m_pm <- cpt.mean(diff[2500:3199], penalty = "Manual", pen.value = "1.5 * log(n)",
method = "PELT")
plot(m_pm, type = "l", cpt.col = "blue", xlab = "Index", cpt.width = 4)
#transform changepoints in dates
#we just sum it from the first day
cpt_date = btc_hist$timestamp[2499] + cpts(m_binseg)
tweet_date = btc_hist %>% filter(tweet == TRUE) %>% select(timestamp)
as.data.frame(cpt_date)
library(stringr)
sum(str_count(elon$tweet, "\\w+"))
elon$tweet
View(eloncrypto)
elondoge = elon[grepl("doge", elon[["tweet"]])
elondoge = elon[grepl("doge", elon[["tweet"]])]
elondoge = elon[grepl("doge", elon[["tweet"]]),]
View(elondoge)
elonCryptoOnly = elon[grepl("crypto", elon[["tweet"]]),]
elonBTC = elon[grepl("bitcoin", elon[["tweet"]]),]
elonBTC = elon[grepl("BTC", elon[["tweet"]]),]
View(elondoge)
count(elondoge$tweet)
countrows(elondoge$tweet)
dim(elondoge)
nrows(elondoge)
count(elondoge$tweet)
elondoge$tweet = as.numeric(elondoge$tweet)
elondoge = elon[grepl("doge", elon[["tweet"]]),]
View(elonCryptoOnly)
length(elondoge$tweet)
n_doge = length(elondoge$tweet)
n_onlycrypto = length(elonCryptoOnly$tweet)
n_BTC = length(elonCryptoOnly$tweet)
n_dogelikes = sum(elondoge$nlikes)
n_onlycrptolikes = sum(elonCryptoOnly$nlikes)
n_BTClike = sum(elonBTC$nlikes)
n_totaltweet = length(elon$tweet)
#subsetting in order to undersand tweet share
library(ggplot2)
vectorshare = c(n_doge, n_onlycrypto, n_BTC, n_totaltweet)
vectorshare= as.data.frame(vectorshare)
View(vectorshare)
n_totaltweet = n_doge + n_onlycrypto + n_BTC
vectorshare = c(n_doge, n_onlycrypto, n_BTC, n_totaltweet)
vectorshare= as.data.frame(vectorshare)
View(vectorshare)
View(vectorshare)
colnames(vectorshare[1]) = "type"
colnames(vectorshare) = c("type")
rownames(vectorshare) = c("doge, crypto, BTC, total")
dim(vectorshare)
rownames(vectorshare) = c("doge", "crypto", "BTC", "total")
plot(vectorshare)
# stacked bar
pie(vectorshare)
# stacked bar
pie(vectorshare)
# stacked bar
pie(vectorshare, x=type)
#counting total nlikes per each keyword
n_dogelikes = sum(elondoge$nlikes)/sum(elon$nlikes)
#counting total nlikes per each keyword
n_dogelikes = sum(elondoge$nlikes)/sum(elon$nlikes)*100
#counting total nlikes per each keyword
n_dogelikes = sum(elondoge$nlikes)/n_doge
n_onlycrptolikes = sum(elonCryptoOnly$nlikes)/n_onlycrypto
n_BTClike = sum(elonBTC$nlikes)/n_BTC
##retweet
n_dogeretweet = sum(elondoge$nretweets)/n_doge
n_BTCretweet = sum(elonBTC$nretweets)/n_BTC
##reply
n_dogereply = sum(elondoge$nreplies)/n_doge
n_onlycrptoreply = sum(elonCryptoOnly$nreplies)/n_onlycrypto
n_BTClreply= sum(elonBTC$nreplies)/n_BTC
#dataframe activity
vectoractivity_doge= c(n_dogelikes, n_dogereply, n_dogeretweet)
vectoractivity_BTC= c(n_BTClike, n_BTClreply, n_BTCretweet)
n_onlycrptoretweet = sum(elonCryptoOnly$nretweets)/n_onlycrypto
vectoractivity_crypto = c(n_onlycrptolikes, n_onlycrptoreply, n_onlycrptoretweet)
dfactivity = c(vectoractivity_BTC, vectoractivity_crypto, vectoractivity_doge)
dfactivity= as.data.frame(dfactivity)
colnames(vectorshare) = c("number")
colnames(vectorshare) = c("tweet number")
vectorshare = c(n_doge, n_onlycrypto, n_BTC)
vectorshare= as.data.frame(vectorshare)
dim(vectorshare)
rownames(vectorshare) = c("doge", "crypto", "BTC", "total")
rownames(vectorshare) = c("doge", "crypto", "BTC")
dfactivity = c(vectoractivity_doge, vectoractivity_crypto, vectoractivity_BTC)
dfactivity= as.data.frame(dfactivity)
df2 = merge(vectorshare, dfactivity)
View(df2)
View(dfactivity)
View(dfactivity)
dfactivity = cbind(vectoractivity_doge, vectoractivity_crypto, vectoractivity_BTC)
View(dfactivity)
dfactivity= as.data.frame(dfactivity)
View(dfactivity)
rownames(dfactivity) = c("doge", "crypto", "BTC")
rownames(dfactivity) = c("like", "replies", "tweets")
rownames(dfactivity) = c("like", "replies", "retweets")
rownames(dfactivity) = c("Like", "Replies", "Retweets")
colnames(dfactivity) = c("Activity Doge", "Activity Crypto", "Activity BTC")
#plotting
barplot(dfactivity)
dfactivity = c(vectoractivity_doge, vectoractivity_crypto, vectoractivity_BTC)
dfactivity= as.data.frame(dfactivity)
dfactivity = cbind(vectoractivity_doge, vectoractivity_crypto, vectoractivity_BTC)
dfactivity= as.data.frame(dfactivity)
rownames(dfactivity) = c("Like", "Replies", "Retweets")
colnames(dfactivity) = c("Activity Doge", "Activity Crypto", "Activity BTC")
dfactivity = cbind(vectoractivity_doge, vectoractivity_crypto, vectoractivity_BTC)
dfactivity= as.data.frame(dfactivity)
rownames(dfactivity) = c("Like", "Replies", "Retweets")
colnames(dfactivity) = c("Activity Doge", "Activity Crypto", "Activity BTC")
#plotting
barplot(height = dfactivity$`Activity Doge`, names=Likes, col=rgb(0.2,0.4,0.6,))
#plotting
barplot(height = Activity Doge, names=Likes, col=rgb(0.2,0.4,0.6,))
View(elon)
View(btc_hist)
tweet2020 = elon
tweet2020 = tweet2020[-c(1:2762),]
View(tweet2020)
corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:5])
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
cleanset1 <- tm_map(corpus1, removeWords, stopwords('english'))
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])
library(tm)
library(RColorBrewer)
library(wordcloud)
tweet2020 = elon
tweet2020 = tweet2020[-c(1:2762),]
corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL1))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])
corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
cleanset1 <- tm_map(corpus1, removeWords, stopwords('english'))
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL1))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])
tdm1 <- TermDocumentMatrix(cleanset1)
tdm1 <- as.matrix(tdm1)
tdm1[1:10, 1:20]
words1 <- rowSums(tdm1)
words1 <- subset(words1, words1>=100)
barplot(words1,
las = 2,
col = rainbow(50))
library(wordcloud)
w1 <- sort(rowSums(tdm1), decreasing = TRUE)
set.seed(222)
wordcloud(words1 = names(w1),
freq = w1,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
wordcloud(words1 = names(w1),
freq = w1,
max.words1 = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
wordcloud(words = names(w1),
freq = w1,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
#eliminate tweet with grepl
tweet2020 = tweet2020[!grepl("amp", tweet2020["tweet"]]),]
#eliminate tweet with grepl
tweet2020 = tweet2020[!grepl("amp", tweet2020[["tweet"]]),]
corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
cleanset1 <- tm_map(corpus1, removeWords, stopwords('english'))
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL1))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])
tdm1 <- TermDocumentMatrix(cleanset1)
tdm1 <- as.matrix(tdm1)
tdm1[1:10, 1:20]
words1 <- rowSums(tdm1)
words1 <- subset(words1, words1>=100)
barplot(words1,
las = 2,
col = rainbow(50))
library(wordcloud)
w1 <- sort(rowSums(tdm1), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w1),
freq = w1,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
library(wordcloud2)
w1 <- data.frame(names(w1), w1)
colnames(w1) <- c('word', 'freq')
wordcloud2(w1,
size = 0.7,
shape = 'rectangle',
rotateRatio = 0.5,
minSize = 1)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
s1 <- get_nrc_sentiment(tweet2020$tweet)
s1 <- get_nrc_sentiment(tweet2020$tweet)
head(s1)
barplot(colSums(s1),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores Tweets')
library(devtools)
txt1 <- tweet2020$tweet
#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt1 <- sapply(txt1,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt1 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt1)
# remove at people
txt1 = gsub("@\\w+", "", txt1)
# remove punctuation
txt1 = gsub("[[:punct:]]", "", txt1)
# remove numbers
txt1 = gsub("[[:digit:]]", "", txt1)
# remove html links
txt1 = gsub("http\\w+", "", txt1)
# remove unnecessary spaces
txt1 = gsub("[ \t]{2,}", "", txt1)
txt1 = gsub("^\\s+|\\s+$", "", txt1)
txt1 = txt1[!is.na(txt1)]
names(txt1) = NULL
library(sentimentr)
sentiment_by(txt1)
t1 = extract_sentiment_terms(txt1)
t1 = extract_sentiment_terms(txt1)
attributes(t1)$count
library(e1071)
library(e1071)
txt1 %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti1
densitySentiments1 <- density(senti1$sentiment)
plot(densitySentiments1,main='Density of sentiments')
polygon(densitySentiments1,col='green')
e1<-emotion_by(get_sentences(txt1),drop.unused.emotions=TRUE)
plot(e1)
