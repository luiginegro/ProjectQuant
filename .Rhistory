<<<<<<< HEAD
btc_hist$timestamp <- format(as.POSIXct(btc_hist$timestamp,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
btc_hist$timestamp <- as.Date(as.character(btc_hist$timestamp), format='%m/%d/%Y')
#subset tweets keeping those that contains ("bitcoin","crypto")
eloncrypto <- elon[grepl("crypto", elon[["tweet"]]) | grepl("BTC", elon[["tweet"]]),]
as.data.frame(eloncrypto)
eloncrypto$date <- format(as.POSIXct(eloncrypto$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
eloncrypto$date <- as.Date(as.character(eloncrypto$date), format='%m/%d/%Y')
btc_hist['tweet'] <- btc_hist$timestamp %in% eloncrypto$date
btc_hist %>%
ggplot(aes(x = timestamp, y = open)) +
geom_line() +
geom_point(data = . %>% filter(tweet == TRUE), color = "deeppink", size = 3) +
ggtitle("Elon Musk's Tweets and Bitcoin Volatility") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_minimal()
=======
scale_fill_gradient2(low = "yellow", high = "red") +
coord_flip() +
labs(title="Sentiment Scores Tweets", x="emotions") +
theme_minimal()
#!all the code will be needed for the presentation, therefore i write all the lines so to use the plot at the end
## MODEL 1
corpus <- iconv(elon$tweet)
corpus <- Corpus(VectorSource(corpus))
if(!require(dplyr)) install.packages("dplyr")
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(readxl)) install.packages("readxl")
library(dplyr)
library(tidyverse)
library(ggplot2)
if(!require(hrbrthemes)) install.packages("hrbrthemes")
library(hrbrthemes)
#install.packages("ggthemes")
library(ggthemes)
#library(readxl)
library(tidyr)
library(crypto2)
library(lubridate)
library(stringr)
if(!require(tm)) install.packages("tm")
library(tm)
library(RColorBrewer)
library(wordcloud)
if(!require(wordcloud2)) install.packages("wordcloud2")
library(wordcloud2)
library(syuzhet)
library(scales)
library(reshape2)
library(devtools)
library(sentimentr)
library(e1071)
library(gridExtra)
if(!require(tseries)) install.packages("tseries")
options(warn=-1)
library(tseries)
if(!require(changepoint)) install.packages("changepoint")
library(changepoint)
library(ggpubr)
elon =  read.csv("elon.csv")
dim(elon)
head(elon)
#create a dataset for analysing popularity
popularity = as.data.frame(cbind(elon$tweet, elon$nlikes, elon$nreplies, elon$nretweets, elon$date))
#change the col names
colnames(popularity) = c("tweet", "nlikes","nreplies", "nretweets", "date")
#Showing summaries of each variable
#install.packages("hrbrthemes")
popularity$date <- format(as.POSIXct(popularity$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
popularity$date <- as.Date(as.character(popularity$date), format='%m/%d/%Y')
popularity$nreplies= as.numeric(popularity$nreplies)
popularity$nretweets= as.numeric(popularity$nretweets)
popularity$nlikes= as.numeric(popularity$nlikes)
#grouping by year
tweets_year<- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(tweet=n())
likes_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nlikes=sum(nlikes))
retweet_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nretweets = sum(nretweets))
nreplies_year <- popularity %>%
mutate(dates = as.Date(popularity$date)) %>%
mutate(yr = format(dates, '%Y')) %>%
group_by(yr) %>%
summarise(nreplies = sum(nreplies))
#removing year 2022
nreplies_year= nreplies_year[-c(12), ]
retweet_year= retweet_year[-c(12), ]
likes_year= likes_year[-c(12), ]
tweets_year = tweets_year[-c(12), ]
Replies = nreplies_year$nreplies
Likes =  likes_year$nlikes
Retweets = retweet_year$nretweets
summary(cbind(Replies, Retweets, Likes))
#numeric
nreplies_year$nreplies= as.numeric(nreplies_year$nreplies)
nreplies_year$yr= as.numeric(nreplies_year$yr)
retweet_year$yr = as.numeric(retweet_year$yr)
retweet_year$nretweets = as.numeric(retweet_year$nretweets)
likes_year$yr = as.numeric(likes_year$yr)
likes_year$nlikes = as.numeric(likes_year$nlikes)
tweets_year$yr = as.numeric(tweets_year$yr)
tweets_year$tweet = as.numeric(tweets_year$tweet)
#plot
library(ggplot2)
tweets <- ggplot(data = tweets_year, aes(x = yr, y = tweet)) +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "purple", color = "black") +
xlab("Year") +
ylab("Number of tweets")  +
ggtitle("Tweets per year") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_minimal()
likes <- ggplot(data = likes_year, aes(x = yr, y = nlikes)) +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "darkorange", color = "black") +
xlab("Year") +
ylab("Number of likes")  +
ggtitle("Likes per year") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_minimal()
retweets <- ggplot(data =retweet_year, aes(x = yr, y = nretweets), color ="darkorange") +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill = "dark green", color = "black") +
xlab("Year") +
ylab("Number of retweets") +
ggtitle("Retweets per year") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_minimal()
replies <- ggplot(data = nreplies_year, aes(x = yr, y = nreplies)) +
geom_bar(stat = "identity", width = 0.5, position = "dodge", fill ="dark red", color ="black") +
xlab("Year") +
ylab("Number of replies")  +
ggtitle("Replies per year") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_minimal()
ggarrange(tweets, likes, retweets, replies,
ncol = 2, nrow = 2)
#!all the code will be needed for the presentation, therefore i write all the lines so to use the plot at the end
## MODEL 1
>>>>>>> 6be810c586ed74e33183fc92fb12423cf5a4aaab
corpus <- iconv(elon$tweet)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
#clean
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
cleanset <- tm_map(corpus, removeWords, stopwords('english'))
removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:10])
#stemming = reduce to root form
cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])
tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]
words <- rowSums(tdm)
words <- subset(words, words>=120)
words <- as.data.frame(words)
words$names <- rownames(words)
#scores and density
s <- get_nrc_sentiment(elon$tweet)
head(s)
s_graph <- colSums(s)
s_graph <- as.data.frame(s_graph)
s_graph$emotions <- rownames(s_graph)
s_graph$count <- s_graph$s_graph
#sentiment
#install_github("trinker/sentimentr")
txt <- elon$tweet
#txt cleaning
#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt <- sapply(txt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt)
# remove at people
txt = gsub("@\\w+", "", txt)
# remove punctuation
txt = gsub("[[:punct:]]", "", txt)
# remove numbers
txt = gsub("[[:digit:]]", "", txt)
# remove html links
txt = gsub("http\\w+", "", txt)
# remove unnecessary spaces
txt = gsub("[ \t]{2,}", "", txt)
txt = gsub("^\\s+|\\s+$", "", txt)
txt = txt[!is.na(txt)]
names(txt) = NULL
sentiment_by(txt)
t = extract_sentiment_terms(txt)
attributes(t)$count
#density plot
txt %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti
#summary statistics model I
densitySentiments <- density(senti$sentiment)
meanSentiTot= mean(senti$sentiment)
sdSentiTot= sd(senti$sentiment)
skewnessSentiTot= skewness(senti$sentiment)
kurtosisSentiTot= kurtosis(senti$sentiment)
IQR = IQR(senti$sentiment)
#table statistics model I
summarizeStatSenti = c(meanSentiTot, sdSentiTot, IQR, skewnessSentiTot, kurtosisSentiTot)
summarizeStatSenti = data.frame((summarizeStatSenti))
rownames(summarizeStatSenti) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti) = "Statistical summary sentiment"
summarizeStatSenti
## MODEL 2
tweet2020 = elon
tweet2020 = tweet2020[-c(1:2762),]
#eliminate tweet containing "amp" with grepl
tweet2020 = tweet2020[!grepl("amp", tweet2020[["tweet"]]),]
corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
cleanset1 <- tm_map(corpus1, removeWords, stopwords('english'))
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL1))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])
#words
tdm1 <- TermDocumentMatrix(cleanset1)
tdm1 <- as.matrix(tdm1)
tdm1[1:10, 1:20]
words1 <- rowSums(tdm1)
words1 <- subset(words1, words1>=60)
words1 <- as.data.frame(words1)
words1$names <- rownames(words1)
#density
s1 <- get_nrc_sentiment(tweet2020$tweet)
head(s1)
s1_graph <- colSums(s1)
s1_graph <- as.data.frame(s1_graph)
s1_graph$emotions <- rownames(s1_graph)
s1_graph$count <- s1_graph$s1_graph
txt1 <- tweet2020$tweet
#txt cleaning
#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt1 <- sapply(txt1,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt1 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt1)
# remove at people
txt1 = gsub("@\\w+", "", txt1)
# remove punctuation
txt1 = gsub("[[:punct:]]", "", txt1)
# remove numbers
txt1 = gsub("[[:digit:]]", "", txt1)
# remove html links
txt1 = gsub("http\\w+", "", txt1)
# remove unnecessary spaces
txt1 = gsub("[ \t]{2,}", "", txt1)
txt1 = gsub("^\\s+|\\s+$", "", txt1)
txt1 = txt1[!is.na(txt1)]
names(txt1) = NULL
sentiment_by(txt1)
t1 = extract_sentiment_terms(txt1)
attributes(t1)$count
#txt
txt1 %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti1
#statstical summary model II
densitySentiments1 <- density(senti1$sentiment)
meanSentiTot1= mean(senti1$sentiment)
sdSentiTot1= sd(senti1$sentiment)
skewnessSentiTot1= skewness(senti1$sentiment)
kurtosisSentiTot1= kurtosis(senti1$sentiment)
IQR1 = IQR(senti1$sentiment)
summarizeStatSenti1 = c(meanSentiTot1, sdSentiTot1, IQR1, skewnessSentiTot1, kurtosisSentiTot1)
summarizeStatSenti1 = data.frame(summarizeStatSenti1)
rownames(summarizeStatSenti1) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti1) = "Statistical summary sentiment"
summarizeStatSenti2 = bind_cols(summarizeStatSenti1, summarizeStatSenti)
colnames(summarizeStatSenti2) = c("Sentiment model I", "Sentiment model II")
summarizeStatSenti2
#plot most used words two models
sentiwords1 <- ggplot(data=words, aes(x=reorder(names, words), y=words)) +
labs(y = "Words", x = "Count", title = "Word frequency Model I") +
geom_bar(stat="identity", aes(fill=words)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
theme_minimal()
<<<<<<< HEAD
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(25, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
w <- data.frame(names(w), w)
colnames(w) <- c('word', 'freq')
wordcloud2(w,
size = 0.7,
shape = 'rectangle',
rotateRatio = 0.5,
minSize = 1)
s <- get_nrc_sentiment(elon$tweet)
head(s)
s_graph <- colSums(s)
s_graph <- as.data.frame(s_graph)
s_graph$emotions <- rownames(s_graph)
s_graph$count <- s_graph$s_graph
ggplot(data=s_graph, aes(x=reorder(emotions, count), y=count)) +
=======
sentiwords2 <-ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
labs(y = "Words", x = "Count", title = "Word frequency Model II") +
geom_bar(stat="identity", aes(fill=words1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
theme_minimal()
ggarrange(sentiwords1, sentiwords2, ncol =2, nrow = 1)
scores1 <- ggplot(data=s_graph, aes(x=reorder(emotions, count), y=count)) +
>>>>>>> 6be810c586ed74e33183fc92fb12423cf5a4aaab
geom_bar(stat="identity", aes(fill=count)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "yellow", high = "red") +
coord_flip() +
labs(title="Sentiment Scores Tweets", x="emotions") +
theme_minimal()
<<<<<<< HEAD
#install_github("trinker/sentimentr")
txt <- elon$tweet
#txt cleaning
#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt <- sapply(txt,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt)
# remove at people
txt = gsub("@\\w+", "", txt)
# remove punctuation
txt = gsub("[[:punct:]]", "", txt)
# remove numbers
txt = gsub("[[:digit:]]", "", txt)
# remove html links
txt = gsub("http\\w+", "", txt)
# remove unnecessary spaces
txt = gsub("[ \t]{2,}", "", txt)
txt = gsub("^\\s+|\\s+$", "", txt)
txt = txt[!is.na(txt)]
names(txt) = NULL
sentiment_by(txt)
t = extract_sentiment_terms(txt)
attributes(t)$count
#Sentiment density plot
txt %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti
#statistic summary
densitySentiments <- density(senti$sentiment)
meanSentiTot= mean(senti$sentiment)
sdSentiTot= sd(senti$sentiment)
skewnessSentiTot= skewness(senti$sentiment)
kurtosisSentiTot= kurtosis(senti$sentiment)
IQR = IQR(senti$sentiment)
summarizeStatSenti = c(meanSentiTot, sdSentiTot, IQR, skewnessSentiTot, kurtosisSentiTot)
summarizeStatSenti = data.frame((summarizeStatSenti))
rownames(summarizeStatSenti) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti) = "Statistical summary sentiment"
summarizeStatSenti
plot(densitySentiments,main='Density of sentiments')
polygon(densitySentiments,col='red')
e<-emotion_by(get_sentences(txt),drop.unused.emotions=TRUE)
plot(e)
#subset after Jan 2020
tweet2020 = elon
tweet2020 = tweet2020[-c(1:2762),]
#eliminate tweet containing "amp" with grepl
tweet2020 = tweet2020[!grepl("amp", tweet2020[["tweet"]]),]
corpus1 <- iconv(tweet2020$tweet)
corpus1 <- Corpus(VectorSource(corpus1))
inspect(corpus1[1:6])
#clean
corpus1 <- tm_map(corpus1, tolower)
corpus1<- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1, removeNumbers)
cleanset1 <- tm_map(corpus1, removeWords, stopwords('english'))
removeURL1 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset1 <- tm_map(cleanset1, content_transformer(removeURL1))
inspect(cleanset1[1:10])
#stemming = reduce to root form
cleanset1 <- tm_map(cleanset1, stripWhitespace)
inspect(cleanset1[1:5])
tdm1 <- TermDocumentMatrix(cleanset1)
tdm1 <- as.matrix(tdm1)
tdm1[1:10, 1:20]
words1 <- rowSums(tdm1)
words1 <- subset(words1, words1>=60)
words1 <- as.data.frame(words1)
words1$names <- rownames(words1)
ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
geom_bar(stat="identity", aes(fill=words1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
theme_minimal()
w1 <- sort(rowSums(tdm1), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w1),
freq = w1,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
w1 <- data.frame(names(w1), w1)
colnames(w1) <- c('word', 'freq')
wordcloud2(w1,
size = 0.7,
shape = 'rectangle',
rotateRatio = 0.5,
minSize = 1)
s1 <- get_nrc_sentiment(tweet2020$tweet)
head(s1)
s1_graph <- colSums(s1)
s1_graph <- as.data.frame(s1_graph)
s1_graph$emotions <- rownames(s1_graph)
s1_graph$count <- s1_graph$s1_graph
ggplot(data=s1_graph, aes(x=reorder(emotions, count), y=count)) +
=======
scores2 <- ggplot(data=s1_graph, aes(x=reorder(emotions, count), y=count)) +
geom_bar(stat="identity", aes(fill=count)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "yellow", high = "red") +
coord_flip() +
labs(title="Sentiment Scores Tweets", x="emotions") +
theme_minimal()
ggarrange(scores1, scores2, ncol =2, nrow = 1)
scores1 <- ggplot(data=s_graph, aes(x=reorder(emotions, count), y=count)) +
>>>>>>> 6be810c586ed74e33183fc92fb12423cf5a4aaab
geom_bar(stat="identity", aes(fill=count)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "yellow", high = "red") +
coord_flip() +
<<<<<<< HEAD
labs(title="Sentiment Scores Tweets",y = "Count", x="emotions") +
theme_minimal()
#install_github("trinker/sentimentr")
txt1 <- tweet2020$tweet
#txt cleaning
#txt <- sapply(txt, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#for Windows based OS
txt1 <- sapply(txt1,function(row) iconv(row, "latin1", "ASCII", sub=""))
#remove punctuation
txt1 = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txt1)
# remove at people
txt1 = gsub("@\\w+", "", txt1)
# remove punctuation
txt1 = gsub("[[:punct:]]", "", txt1)
# remove numbers
txt1 = gsub("[[:digit:]]", "", txt1)
# remove html links
txt1 = gsub("http\\w+", "", txt1)
# remove unnecessary spaces
txt1 = gsub("[ \t]{2,}", "", txt1)
txt1 = gsub("^\\s+|\\s+$", "", txt1)
txt1 = txt1[!is.na(txt1)]
names(txt1) = NULL
sentiment_by(txt1)
t1 = extract_sentiment_terms(txt1)
attributes(t1)$count
txt1 %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti1
#statistic summary
densitySentiments1 <- density(senti1$sentiment)
meanSentiTot1= mean(senti1$sentiment)
sdSentiTot1= sd(senti1$sentiment)
skewnessSentiTot1= skewness(senti1$sentiment)
kurtosisSentiTot1= kurtosis(senti1$sentiment)
IQR1 = IQR(senti1$sentiment)
summarizeStatSenti1 = c(meanSentiTot1, sdSentiTot1, IQR1, skewnessSentiTot1, kurtosisSentiTot1)
summarizeStatSenti1 = data.frame(summarizeStatSenti1)
rownames(summarizeStatSenti1) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti1) = "Statistical summary sentiment"
summarizeStatSenti2 = bind_cols(summarizeStatSenti1, summarizeStatSenti)
colnames(summarizeStatSenti2) = c("Sentiment model I", "Sentiment model II")
summarizeStatSenti2
plot(densitySentiments1,main='Density of sentiments')
polygon(densitySentiments1,col='green')
e1<-emotion_by(get_sentences(txt1),drop.unused.emotions=TRUE)
plot(e1)
par(mfrow=c(2,1))
plot(btc_hist$timestamp,btc_hist$open,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(btc_hist$open,lag.max = length(btc_hist$open),
xlab = "lag #", ylab = 'ACF', main=' ')
adf.test(btc_hist$open)
#data non stationary as high p-value
par(mfrow=c(2,1))
btc_hist['log'] = log(btc_hist$open)
plot(btc_hist$timestamp,btc_hist$log,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(btc_hist$log,lag.max = length(btc_hist$log),
xlab = "lag #", ylab = 'ACF', main=' ')
par(mfrow=c(2,1))
logdiff = diff(btc_hist$log)
plot(btc_hist$timestamp[2:3200],logdiff,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(logdiff,lag.max = length(btc_hist$open),
xlab = "lag #", ylab = 'ACF', main=' ')
adf.test(logdiff)
m_binseg <- cpt.mean(logdiff, penalty = "BIC", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
#all the changes happen from 2500 onwards approx, try to subset plot
m_binseg <- cpt.mean(logdiff[2500:3199], penalty = "BIC", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
#transform changepoints in dates
#we just sum it from the first day
cpt_date = btc_hist$timestamp[2499] + cpts(m_binseg)
tweet_date = btc_hist %>% filter(tweet == TRUE) %>% select(timestamp)
eloncrypto <- elon[grepl("crypto", elon[["tweet"]]) | grepl("BTC", elon[["tweet"]]),]
as.data.frame(eloncrypto)
cpt_date <- format(as.POSIXct(cpt_date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
cpt_date <- as.Date(as.character(cpt_date), format='%m/%d/%Y')
btc_hist['breakpoint'] <- btc_hist$timestamp %in% cpt_date
doge <- crypto_list(only_active=TRUE)
doge <- subset(doge, doge$slug == "dogecoin")
doge_hist <- crypto_history(doge, limit=1, start_date="20200101", end_date="20220131")
as.data.frame(doge_hist)
doge_hist$timestamp <- format(as.POSIXct(doge_hist$timestamp,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
doge_hist$timestamp <- as.Date(as.character(doge_hist$timestamp), format='%m/%d/%Y')
par(mfrow=c(2,3))
plot(doge_hist$timestamp,doge_hist$open,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(doge_hist$open,lag.max = length(doge_hist$timestamp),
xlab = "lag #", ylab = 'ACF', main=' ')
dogelogdiff = diff(log(doge_hist$open))
plot(doge_hist$timestamp[2:760],dogelogdiff,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(dogelogdiff,lag.max = length(doge_hist$timestamp),
xlab = "lag #", ylab = 'ACF', main=' ')
doge_m_binseg <- cpt.mean(dogelogdiff, penalty = "None", method = "BinSeg", Q = 15)
plot(doge_m_binseg, type = "l", xlab = "Index", cpt.width = 4)
doge_cpt_date = doge_hist$timestamp[1] + cpts(doge_m_binseg)
doge_cpt_date <- format(as.POSIXct(doge_cpt_date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
doge_cpt_date <- as.Date(as.character(doge_cpt_date), format='%m/%d/%Y')
doge_hist['breakpoint'] <- doge_hist$timestamp %in% doge_cpt_date
dogecrypto <- elon[grepl("Doge", elon[["tweet"]]) | grepl("dogecoin", elon[["tweet"]]),]
as.data.frame(dogecrypto)
dogecrypto$date <- format(as.POSIXct(dogecrypto$date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
dogecrypto$date <- as.Date(as.character(dogecrypto$date), format='%m/%d/%Y')
doge_hist['tweet'] <- doge_hist$timestamp %in% dogecrypto$date
colorsD <- c("tweets" = "green",  "breaking points" = "orange")
doge_hist[300:760,] %>%
ggplot(aes(x = timestamp, y = open)) +
geom_line() +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3) +
geom_point(data = . %>% filter(breakpoint == TRUE), aes(color = "breaking points"), size = 3) +
ggtitle("Elon Musk's Tweets and Breaking Points") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(x = "timestamp",
y = "DOGE value",
color = "Legend") + scale_color_manual(values = colorsD) +
theme_minimal()
colors <- c("tweets" = "green",  "breaking points" = "orange")
prova = btc_hist[2750:3200,] %>% filter(breakpoint == TRUE)
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) +
labs(x = "timestamp",
y = "DOGE value",
color = "Legend") + scale_color_manual(values = colorsD) +
geom_line() +
geom_point(data = prova, color ="tweets", size = 3)
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3) +
geom_point(data = . %>% filter(breakpoint == TRUE), aes(color = "breaking points"), size = 3) +
ggtitle("Elon Musk's Tweets and Breaking Points") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(x = "timestamp",
y = "DOGE value",
color = "Legend") + scale_color_manual(values = colorsD) +
theme_minimal()
colors <- c("tweets" = "blue",  "breaking points" = "red")
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3) +
geom_point(data = . %>% filter(breakpoint == TRUE), aes(color = "breaking points"), size = 3) +
ggtitle("Elon Musk's Tweets and Breaking Points") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(x = "timestamp",
y = "DOGE value",
color = "Legend") + scale_color_manual(values = colorsD) +
theme_minimal()
colors <- c("tweets" = "blue",  "breakingpoints" = "red")
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3) +
geom_point(data = . %>% filter(breakpoint == TRUE), aes(color = "breakingpoints"), size = 3) +
ggtitle("Elon Musk's Tweets and Breaking Points") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(x = "timestamp",
y = "DOGE value",
color = "Legend") + scale_color_manual(values = colorsD) +
theme_minimal()
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3)
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) + geom_line() +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3)
#all the changes happen from 2500 onwards approx, try to subset plot
m_binseg <- cpt.mean(logdiff[2500:3199], penalty = "None", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
#all the changes happen from 2750 onwards approx, try to subset plot
m_binseg <- cpt.mean(logdiff[2750:3199], penalty = "None", method = "BinSeg", Q = 15)
plot(m_binseg, type = "l", xlab = "Index", cpt.width = 4)
cpt_date = btc_hist$timestamp[2749] + cpts(m_binseg)
tweet_date = btc_hist %>% filter(tweet == TRUE) %>% select(timestamp)
eloncrypto <- elon[grepl("crypto", elon[["tweet"]]) | grepl("BTC", elon[["tweet"]]),]
as.data.frame(eloncrypto)
cpt_date <- format(as.POSIXct(cpt_date,format="%Y-%m-%d %H:%M:%OS"),format='%m/%d/%Y')
cpt_date <- as.Date(as.character(cpt_date), format='%m/%d/%Y')
btc_hist['breakpoint'] <- btc_hist$timestamp %in% cpt_date
colors <- c("tweets" = "blue",  "breakingpoints" = "red")
btc_hist[2750:3200,] %>%
ggplot(aes(x = timestamp, y = open)) + geom_line() +
geom_point(data = . %>% filter(tweet == TRUE), aes(color = "tweets"), size = 3) +
geom_point(data = . %>% filter(breakpoint == TRUE), aes(color = "breakingpoints"), size = 3) +
ggtitle("Elon Musk's Tweets and Breaking Points") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(x = "timestamp",
y = "DOGE value",
color = "Legend") + scale_color_manual(values = colorsD) +
=======
labs(title="Sentiment Scores Tweets", x="emotions") +
theme_minimal()
scores2 <- ggplot(data=s1_graph, aes(x=reorder(emotions, count), y=count)) +
geom_bar(stat="identity", aes(fill=count)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "yellow", high = "red") +
coord_flip() +
labs(title="Sentiment Scores Tweets", x="emotions") +
theme_minimal()
ggarrange(scores1, scores2, ncol =2, nrow = 1)
par(mfrow=c(2,1))
plot(densitySentiments,main='Density of sentiments Model I')
polygon(densitySentiments,col='red')
plot(densitySentiments1,main='Density of sentiments Model II')
polygon(densitySentiments1,col='green')
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
polygon(densitySentiments,col='red')
plot(densitySentiments1,main='Density of sentiments Model II')
polygon(densitySentiments1,col='green')
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
polygon(densitySentiments,col='red')
plot(densitySentiments1,main='Density of sentiments Model II')
polygon(densitySentiments1,col='green')
meanSentiTot1= mean(senti1$sentiment)
meanSentiTot
meanSentiTot1
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
mtext("Mean = 0.1824", side=3)
polygon(densitySentiments,col='red')
plot(densitySentiments1,main='Density of sentiments Model II')
mtext("Mean = 0.1981", side=3)
polygon(densitySentiments1,col='green')
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
mtext("Mean = 0.1824", side=2)
polygon(densitySentiments,col='red')
plot(densitySentiments1,main='Density of sentiments Model II')
mtext("Mean = 0.1981", side=2)
polygon(densitySentiments1,col='green')
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
mtext("Mean = 0.1824", side=1)
polygon(densitySentiments,col='red')
plot(densitySentiments1,main='Density of sentiments Model II')
mtext("Mean = 0.1981", side=1)
polygon(densitySentiments1,col='green')
dfactivity %>%
gather(key, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = key)) +
geom_col(position = "dodge") +
coord_flip() +
theme_minimal()â‰¤
#plotting
dfactivity %>%
gather(key, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = key)) +
geom_col(position = "dodge") +
coord_flip() +
theme_minimal()
dfactivity %>%
gather(key, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = variable)) +
geom_col(position = "dodge", color= "black") +
coord_flip() +
theme_minimal()
#plotting
dfactivity %>%
gather(variable, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = variable)) +
geom_col(position = "dodge", color= "black") +
coord_flip() +
theme_minimal()
dfactivity %>%
gather(variable, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = variable)) +
geom_col(position = "dodge", color= "black") +
coord_flip() +
scale_fill_grey( +)
#plotting
dfactivity %>%
gather(variable, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = variable)) +
geom_col(position = "dodge", color= "black") +
coord_flip() +
scale_fill_grey() +
theme_minimal()
#plotting
dfactivity %>%
gather(variable, value, -Total) %>%
ggplot(aes(x=Total, y=value, fill = variable)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_grey() +
theme_minimal()
txt1 %>%
get_sentences() %>%
sentiment() %>%
filter(sentiment!=0) -> senti1
#statistic summary
densitySentiments1 <- density(senti1$sentiment)
meanSentiTot1= mean(senti1$sentiment)
sdSentiTot1= sd(senti1$sentiment)
skewnessSentiTot1= skewness(senti1$sentiment)
kurtosisSentiTot1= kurtosis(senti1$sentiment)
IQR1 = IQR(senti1$sentiment)
summarizeStatSenti1 = c(meanSentiTot1, sdSentiTot1, IQR1, skewnessSentiTot1, kurtosisSentiTot1)
summarizeStatSenti1 = data.frame(summarizeStatSenti1)
rownames(summarizeStatSenti1) = c("Mean", "Sd", "IQR", "Skewness", "Kurtosis")
colnames(summarizeStatSenti1) = "Statistical summary sentiment"
summarizeStatSenti2 = bind_cols(summarizeStatSenti1, summarizeStatSenti)
colnames(summarizeStatSenti2) = c("Sentiment model I", "Sentiment model II")
summarizeStatSenti2
summarizeStatSenti2
knitr::kable(summarizeStatSenti2, caption = "Models' Descriptive Statistics")
knitr::kable(summarizeStatSenti2, caption = "Models' Descriptive Statistics") %>%
kable_classic(full_width = T, html_font = "Cambria")
knitr::kable(summarizeStatSenti2, caption = "Models' Descriptive Statistics") %>%
kable_classic(full_width = T, html_font = "Cambria")
knitr::kable(summarizeStatSenti2, caption = "Models' Descriptive Statistics") %>%
knitr::kable_classic(full_width = T, html_font = "Cambria")
install.packages("kable")
install.packages("kable_classic")
remotes::install_github("haozhu233/kableExtra")
library(kableExtra)
knitr::kable(summarizeStatSenti2, caption = "Models' Descriptive Statistics")
knitr::kable(summarizeStatSenti2, caption = "Models' Descriptive Statistics") %>%
kable_classic(full_width = T, html_font = "Cambria")
ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
geom_bar(stat="identity", aes(fill=words1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
theme_minimal()
ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
geom_bar(stat="identity", aes(fill=words1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
theme_minimal()
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
polygon(densitySentiments,col='white')
plot(densitySentiments1,main='Density of sentiments Model II')
polygon(densitySentiments1,col='grey')
plot(densitySentiments1,main='Density of sentiments')
par(mfrow=c(1,2))
plot(densitySentiments,main='Density of sentiments Model I')
polygon(densitySentiments,col='white')
plot(densitySentiments1,main='Density of sentiments Model II')
polygon(densitySentiments1,col='grey')
e1<-emotion_by(get_sentences(txt1),drop.unused.emotions=TRUE)
plot(e1)
citr:::insert_citation()
ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
geom_bar(stat="identity", aes(fill=words1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
theme_minimal()
ggplot(data=words1, aes(x=reorder(names, words1), y=words1)) +
geom_bar(stat="identity", aes(fill=words1)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "blue", high = "darkblue") +
coord_flip() +
>>>>>>> 6be810c586ed74e33183fc92fb12423cf5a4aaab
theme_minimal()
ggplot(data=s_graph, aes(x=reorder(emotions, count), y=count)) +
geom_bar(stat="identity", aes(fill=count)) +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_fill_gradient2(low = "white", high = "black") +
coord_flip() +
labs(title="Sentiment Scores Tweets",y = "Count", x="emotions") +
theme_minimal()
par(mfrow=c(2,1))
logdiff = diff(btc_hist$log)
plot(btc_hist$timestamp[2:3200],logdiff,
type='l',col='red',
xlab = "time (t)",
ylab = "Y(t)",
main = "Trend signal")
acf(logdiff,lag.max = length(btc_hist$open),
xlab = "lag #", ylab = 'ACF', main=' ')
<<<<<<< HEAD
install.packages("devtools")
devtools::install_github("crsh/citr")
citr:::insert_citation()
citr:::insert_citation()
> citr:::insert_citation()
citr:::insert_citation()
> citr:::insert_citation()
citr::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
devtools::install_github("mangothecat/rmdshower")
=======
View(e1)
View(densitySentiments)
View(densitySentiments)
View(t1)
View(summarizeStatSenti2)
View(summarizeStatSenti)
View(sentiwords2)
library(reticulate)
install.packages("reticulate")
library(reticulate)
reticulate::repl_python()
Y
use_python("/usr/local/bin/python3.7")
use_python("/usr/local/bin/python3")
library(reticulate)
reticulate::repl_python()
>>>>>>> 6be810c586ed74e33183fc92fb12423cf5a4aaab
